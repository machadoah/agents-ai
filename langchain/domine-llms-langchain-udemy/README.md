# Domine LLMs com LangChain ü¶úüîó

Curso: [Domine LLMs com LangChain](https://www.udemy.com/course/domine-llms-com-langchain)

## Conte√∫do do Curso

- Introdu√ß√£o sobre LLMs
- LLM com Hugging Face ü§ó
- LLM com LangChain ü¶úüîó
- Aplica√ß√£o RAG
- Agentes e Tools
- Projeto 01 : Transcri√ß√£o e compreens√ß√£o de v√≠deos
- Projeto 02 : Chatbot com mem√≥ria e interface gr√°fica (streamlit)
- Projeto 03 : Converse com seus documentos

## 1. Introdu√ß√£o sobre LLMs

### O que s√£o LLMs?

- **LLMs (Large Language Models)** - "grandes modelos de linguagem" ou "Modelos de Linguagem de grande escala" -  s√£o modelos de IA que utilizam t√©cnicas de  Aprendizado de M√°quina para entender e gerar linguagem humana.

- Utilizam Processamento de Linguagem Natural (**NLP**) para interagir com linguagem humana.

- √â importante esclarecer e saber a diferen√ßa entre os conceitos:
  - Intelig√™ncia Artificial (**IA**)
  - Aprendizado de M√°quina / Machine Learning (**ML**)
  - Processamento de Linguagem Natural (**NLP**)
  - Redes Neurais Artificiais (**ANN**)
  - IA Generativa
  - LLM (Large Language Models)

### Como funcionam os LLMs?

-  Utilizam aprendizados n√£o-supervisionados para entender a estrutura da linguagem humana.
- Um modelos de ML √© alimentado com conjuntos de dados de texto para aprender a estrutura da linguagem.
- O modelo aprende com esses exemplos sem instru√ß√£o expl√≠cita.
- Extrai informa√ß√µes dos dados, cria conex√µes e aprende a estrutura da linguagem.
- Como resultado, o modelo captura as rela√ß√µes complexas entre as palavras e frases.
- LLMs requerem recursos computacionais significativos para treinamento e infer√™ncia.
- Utilizam unidades de processamento gr√°fico (**GPU**) para acelerar o treinamento e lidar com c√°lculos complexos.
- O uso de GPUs √© essencial para treinar modelos de linguagem de grande escala, acelera o treinamento e a opera√ß√£o dos transformadores.

![Trasnformadores](boards/como%20funcionam%20os%20llms%20-%20transformadores.excalidraw.png)

#### Transformadores (Transformers)

- **Transformers** s√£o uma arquitetura de rede neural que utiliza mecanismos de aten√ß√£o para processar sequ√™ncias de dados. 
- Transforma ou altera a representa√ß√£o de uma sequ√™ncia de dados em outra sequ√™ncia de dados.
- Foi apresentado no artigo "Attention is All You Need".
- A arquitetura melhora a capacidade dos modelos de ML ao capturar rela√ß√µes de longo alcance.

> [!NOTE]
> Por exemplo:
> Em "Qual √© a cor do c√©u?", o modelo identifica a rela√ß√£o entre "cor", "c√©u" e "azul" para gerar "O c√©u √© azul".

### Embeddings e Tokens

- **Embeddings** s√£o representa√ß√µes vetoriais de palavras ou frases.
- No contexto de LLMs, embeddings s√£o vetores que representam palavras ou frases.

![Embeddings](boards/embedding%20-%20espa√ßo%20vetorial.excalidraw.png)

- O modelo aprende a separar e agrupar esses textos com base em suas similaridades.

> [!NOTE]
> Por exemplo:
> Quando o modelo receber uma nova palavra, como "abacaxi", ele sabe exatamente onde coloc√°-la no espa√ßo vetorial, muito provavelmnete perto de "frutas".

Para palavra como "banco", o modelo pode coloc√°-la perto de "dinheiro" ou "sentar", dai se d√° a importancia do mecanismo de aten√ß√£o.

- **Tokens** s√£o unidades menores de texto criadas atrav√©s da tokeniza√ß√£o.

N√£o obrigatoriamente uma palavra representa um token, pode ser uma parte de uma palavra, ou at√© mesmo um caractere.

